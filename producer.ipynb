{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "electronic-washer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import socket\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "from kafka import KafkaProducer\n",
    "import time\n",
    "from json import dumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "impossible-marketplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_NAME = \"spark\"\n",
    "NORMALIZED_APP_NAME = APP_NAME.replace('/', '_').replace(':', '_')\n",
    "APPS_TMP_DIR = os.path.join(os.getcwd(), \"tmp\")\n",
    "APPS_CONF_DIR = os.path.join(os.getcwd(), \"conf\")\n",
    "APPS_LOGS_DIR = os.path.join(os.getcwd(), \"logs\")\n",
    "LOG4J_PROP_FILE = os.path.join(APPS_CONF_DIR, \"pyspark-log4j-{}.properties\".format(NORMALIZED_APP_NAME))\n",
    "LOG_FILE = os.path.join(APPS_LOGS_DIR, 'pyspark-{}.log'.format(NORMALIZED_APP_NAME))\n",
    "EXTRA_JAVA_OPTIONS = f\"-Dlog4j.configuration=file://{LOG4J_PROP_FILE} -Dspark.hadoop.dfs.replication=1 -Dhttps.protocols=TLSv1.0,TLSv1.1,TLSv1.2,TLSv1.3\"\n",
    "LOCAL_IP = socket.gethostbyname(socket.gethostname())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "excited-steering",
   "metadata": {},
   "outputs": [],
   "source": [
    "for directory in APPS_CONF_DIR, APPS_LOGS_DIR, APPS_TMP_DIR:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "env = Environment(loader=FileSystemLoader('/opt'))\n",
    "template = env.get_template(\"pyspark_log4j.properties.template\")\n",
    "template.stream(logfile=LOG_FILE).dump(LOG4J_PROP_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "capable-reggae",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(APP_NAME).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "resident-projector",
   "metadata": {},
   "outputs": [],
   "source": [
    "followers_info_df = spark.read.json(\"file:///home/jovyan/shared-data/bigdata20/followers_info.json\")\n",
    "followers_posts_api_final_df = spark.read.json(\"file:///home/jovyan/shared-data/bigdata20/followers_posts_api_final.json/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "flying-renaissance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regexp for vk's DD.MM.YYYY bdate patern\n",
    "regexp = \"^([0]?[1-9]|[1|2][0-9]|[3][0|1])[./-]([0]?[1-9]|[1][0-2])[./-]([0-9]{4}|[0-9]{2})$\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "outdoor-steps",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = followers_info_df.join(\n",
    "            followers_posts_api_final_df, \n",
    "            followers_info_df.id==followers_posts_api_final_df.owner_id, \n",
    "            'inner'\n",
    "        ) \\\n",
    "        .filter(col('bdate').rlike(regexp)) \\\n",
    "        .withColumn('date', F.from_unixtime('date')) \\\n",
    "        .where(col('text') != \"\") \\\n",
    "        .select(\n",
    "            'owner_id', \n",
    "            col('sex').alias('owner_sex'), \n",
    "            col('bdate').alias('owner_bdate'), \n",
    "            'date', \n",
    "            'text'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "owned-truck",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+-------------------+--------------------+\n",
      "|owner_id|owner_sex|owner_bdate|               date|                text|\n",
      "+--------+---------+-----------+-------------------+--------------------+\n",
      "|   94494|        2|  31.8.1987|2019-06-04 11:28:50|Тот неловкий моме...|\n",
      "|   94494|        2|  31.8.1987|2019-06-06 20:48:46|Белые ночи + жарк...|\n",
      "|   94494|        2|  31.8.1987|2019-06-16 10:22:14|Не велосипедом ед...|\n",
      "|   94494|        2|  31.8.1987|2019-06-19 23:36:14|Тот самый француз...|\n",
      "|   94494|        2|  31.8.1987|2019-06-21 22:11:22|Да, знатное шоу с...|\n",
      "+--------+---------+-----------+-------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "posts.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "prerequisite-compensation",
   "metadata": {},
   "outputs": [],
   "source": [
    "producer = KafkaProducer(\n",
    "    bootstrap_servers='kafka-svc:9092',\\\n",
    "    value_serializer=lambda data: dumps(data).encode('utf-8') \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "political-transition",
   "metadata": {},
   "outputs": [],
   "source": [
    "for post in posts.rdd.collect():\n",
    "    frame = {'sex': post.owner_sex,\n",
    "            'bdate': post.owner_bdate,\n",
    "            'text': post.text,\n",
    "            'id':post.owner_id}\n",
    "    \n",
    "    producer.send('posts', frame)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupational-palace",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
